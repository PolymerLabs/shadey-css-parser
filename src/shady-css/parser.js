/**
 * @license
 * Copyright (c) 2016 The Polymer Project Authors. All rights reserved.
 * This code may only be used under the BSD style license found at http://polymer.github.io/LICENSE.txt
 * The complete set of authors may be found at http://polymer.github.io/AUTHORS.txt
 * The complete set of contributors may be found at http://polymer.github.io/CONTRIBUTORS.txt
 * Code distributed by Google as part of the polymer project is also
 * subject to an additional IP rights grant found at http://polymer.github.io/PATENTS.txt
 */

import { Tokenizer } from './tokenizer';
import { Token } from './token';
import { NodeFactory } from './node-factory';

/**
 * Class that implements a shady CSS parser.
 */
class Parser {
  /**
   * Create a Parser instance. When creating a Parser instance, a specialized
   * NodeFactory can be supplied to implement streaming analysis and
   * manipulation of the CSS AST.
   */
  constructor(nodeFactory = new NodeFactory()) {
    this.nodeFactory = nodeFactory;
  }

  /**
   * Parse CSS and generate an AST.
   * @param {string} cssText The CSS to parse.
   * @return {object} A CSS AST containing nodes that correspond to those
   * generated by the Parser's NodeFactory.
   */
  parse(cssText) {
    return this.parseStylesheet(new Tokenizer(cssText));
  }

  /**
   * Consumes tokens from a Tokenizer to parse a Stylesheet node.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @return {object} A Stylesheet node.
   */
  parseStylesheet(tokenizer) {
    return this.nodeFactory.stylesheet(this.parseRules(tokenizer));
  }

  /**
   * Consumes tokens from a Tokenizer to parse a sequence of rules.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @return {array} A list of nodes corresponding to rules. For a parser
   * configured with a basic NodeFactory, any of Comment, AtRule, Ruleset,
   * Declaration and Discarded nodes may be present in the list.
   */
  parseRules(tokenizer) {
    let rules = [];

    while (tokenizer.currentToken) {
      let rule = this.parseRule(tokenizer);

      if (rule) {
        rules.push(rule);
      }
    }

    return rules;
  }

  /**
   * Consumes tokens from a Tokenizer to parse a single rule.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @param {boolean=false} allowDeclarations Set to true if a declaration is
   * a valid rule to be parsed in this call.
   * @return {object} If the current token in the Tokenizer is whitespace,
   * returns null. Otherwise, returns the next parseable node.
   */
  parseRule(tokenizer, allowDeclarations = false) {
    // Trim leading whitespace:
    if (tokenizer.currentToken.is(Token.type.whitespace)) {
      tokenizer.advance();
      return null;

    } else if (tokenizer.currentToken.is(Token.type.comment)) {
      return this.parseComment(tokenizer);

    } else if (tokenizer.currentToken.is(Token.type.word)) {
      if (allowDeclarations) {
        return this.parseDeclaration(tokenizer);
      } else {
        return this.parseRuleset(tokenizer);
      }
    } else if (tokenizer.currentToken.is(Token.type.at)) {
      return this.parseAtRule(tokenizer);

    } else {
      return this.parseUnknown(tokenizer);
    }
  }

  /**
   * Consumes tokens from a Tokenizer to parse a Comment node.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @return {object} A Comment node.
   */
  parseComment(tokenizer) {
    return this.nodeFactory.comment(tokenizer.slice(tokenizer.advance()));
  }

  /**
   * Consumes tokens from a Tokenizer through the next boundary token to
   * produce a Discarded node. This supports graceful recovery from many
   * malformed CSS conditions.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @return {object} A Discarded node.
   */
  parseUnknown(tokenizer) {
    let start = tokenizer.advance();
    let end;

    while (tokenizer.currentToken &&
           tokenizer.currentToken.is(Token.type.boundary)) {
      end = tokenizer.advance();
    }

    return this.nodeFactory.discarded(tokenizer.slice(start, end));
  }

  /**
   * Consumes tokens from a Tokenizer to parse an At Rule node.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @return {object} An At Rule node.
   */
  parseAtRule(tokenizer) {
    let name = '';
    let rulelist = null;
    let parametersStart = null;
    let parametersEnd = null;

    while (tokenizer.currentToken) {
      if (tokenizer.currentToken.is(Token.type.whitespace)) {
        tokenizer.advance();
      } else if (!name && tokenizer.currentToken.is(Token.type.at)) {
        // Discard the @:
        tokenizer.advance();
        let start = tokenizer.currentToken;
        let end;
        
        while (tokenizer.currentToken &&
               tokenizer.currentToken.is(Token.type.word)) {
          end = tokenizer.advance();
        }
        name = tokenizer.slice(start, end);
      } else if (tokenizer.currentToken.is(Token.type.openBrace)) {
        // NOTE(cdata): We allow declarations as children of this at-rule iff
        // there are no parameters in this at-rule. This is an attempt at
        // generalizing the format of several standard at-rules (@media,
        // @font-face, @import, @apply etc.), but is not part of the standard
        // grammar and holds no guarantee of being future proof. Examples:
        // @font-face { src: foo; } has no parameters, so allows declarations.
        // @keyframes foo { 0% { bar: baz; } } has parameters (foo), so only
        // allows rulesets (also applies to @media).
        // @apply --foo; technically allows declarations, but won't have an
        // associated rulelist in the common case (also applies to @import).
        rulelist = this.parseRulelist(tokenizer, parametersStart === null);
        break;
      } else if (tokenizer.currentToken.is(Token.type.propertyBoundary)) {
        tokenizer.advance();
        break;
      } else {
        if (parametersStart == null) {
          parametersStart = tokenizer.advance();
        } else {
          parametersEnd = tokenizer.advance();
        }
      }
    }

    return this.nodeFactory.atRule(
        name,
        parametersStart ?
            tokenizer.slice(parametersStart, parametersEnd) : null,
        rulelist);
  }

  /**
   * Consumes tokens from a Tokenizer to produce a Rulelist node.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @param {boolean=false} allowDeclarations Set to true if declarations are
   * valid rules for this rule list to contain.
   * @return {object} A Rulelist node.
   */
  parseRulelist(tokenizer, allowDeclarations = false) {
    let rules = [];

    // Take the opening { boundary:
    tokenizer.advance();

    while (tokenizer.currentToken) {
      if (tokenizer.currentToken.is(Token.type.closeBrace)) {
        tokenizer.advance();
        break;
      } else {
        let rule = this.parseRule(tokenizer, allowDeclarations);
        if (rule) {
          rules.push(rule);
        }
      }
    }

    return this.nodeFactory.rulelist(rules);
  }

  /**
   * Consumes tokens from a Tokenizer to produce a Declaration node.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @return {object} A Declaration node.
   */
  parseDeclaration(tokenizer) {
    let nameStart = tokenizer.advance();
    let nameEnd = nameStart;
    let value = null;

    while (tokenizer.currentToken) {
      if (tokenizer.currentToken.is(Token.type.colon)) {
        tokenizer.advance();

        if (tokenizer.currentToken.is(Token.type.whitespace)) {
          tokenizer.advance();
        }

        if (tokenizer.currentToken.is(Token.type.openBrace)) {
          value = this.parseRulelist(tokenizer, true);
        } else {
          value = this.parseExpression(tokenizer);
        }
      } else if (tokenizer.currentToken.is(Token.type.propertyBoundary)) {
        if (tokenizer.currentToken.is(Token.type.semicolon)) {
          tokenizer.advance();
        }
        break;
      } else if (!value) {
        nameEnd = tokenizer.advance();
      } else {
        break;
      }
    }

    return this.nodeFactory.declaration(
        tokenizer.slice(nameStart, nameEnd), value);
  }

  /**
   * Consumes tokens from a Tokenizer to produce a Ruleset node.
   * @param {Tokenizer} tokenizer A Tokenizer instance.
   * @return {object} A Ruleset node.
   */
  parseRuleset(tokenizer) {
    let selectorStart = tokenizer.advance();
    let selectorEnd = selectorStart;

    while (tokenizer.currentToken) {
      if (tokenizer.currentToken.is(Token.type.openBrace)) {
        break;
      }

      selectorEnd = tokenizer.advance();
    }

    return this.nodeFactory.ruleset(
        tokenizer.slice(selectorStart, selectorEnd),
        this.parseRulelist(tokenizer, true));
  }

  parseExpression(
      tokenizer, terminatingBoundaryType = Token.type.propertyBoundary) {
    let terms = [];

    while (tokenizer.currentToken) {
      if (tokenizer.currentToken.is(Token.type.whitespace)) {
        tokenizer.advance();
        continue;
      }

      if (tokenizer.currentToken.is(terminatingBoundaryType)) {
        break;
      }

      terms.push(this.parseTerm(tokenizer, terminatingBoundaryType));
    }

    return this.nodeFactory.expression(terms);
  }

  parseTerm(tokenizer, terminatingBoundaryType = Token.type.propertyBoundary) {
    let termStart = tokenizer.currentToken;
    let termEnd = termStart;

    while (tokenizer.currentToken) {
      if (tokenizer.currentToken.is(terminatingBoundaryType)) {
        break;
      }

      if (tokenizer.currentToken.is(Token.type.operator)) {
        if (termEnd !== termStart) {
          break;
        }

        let token = tokenizer.advance();

        // NOTE(cdata): + and - operators are required to have whitespace on
        // both sides in order to disambiguate them from idents-with-hyphens and
        // positive or negative dimensions, which are expressed with an
        // explicit + or - prefix.
        if (!token.is(Token.type.additiveOperator) ||
            (tokenizer.currentToken &&
             tokenizer.currentToken.is(Token.type.whitespace))) {
          return this.nodeFactory.operator(tokenizer.slice(token));
        } else {
          termEnd = token;
          continue;
        }
      } else if (tokenizer.currentToken.is(Token.type.openParenthesis)) {
          return this.nodeFactory.function(
              tokenizer.slice(termStart, termEnd),
              this.parseFunctionExpression(tokenizer));
      }

      if (tokenizer.currentToken) {
        termEnd = tokenizer.advance();
      }
    }

    return this.nodeFactory.term(tokenizer.slice(termStart, termEnd));
  }

  parseFunctionExpression(tokenizer) {
    // Take the open parenthesis:
    tokenizer.advance();

    // The call internals are treated as an expression, but semicolons can be
    // ignored:
    let expression = this.parseExpression(tokenizer, Token.type.functionBoundary);

    if (tokenizer.currentToken.is(Token.type.closeParenthesis)) {
      tokenizer.advance();
    }

    return expression;
  }
}

export { Parser };
